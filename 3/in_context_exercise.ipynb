{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FFzQpvN7xuU"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "bHrkuuYG8dla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TriviaQA\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "class QABenchmark:\n",
        "    def __init__(self):\n",
        "        self.dataset = []\n",
        "\n",
        "    def sample(self, k: int):\n",
        "        return random.sample(self.dataset, min(k, len(self.dataset)))\n",
        "    \n",
        "    def first_k(self, k: int):\n",
        "        return self.dataset[:k]\n",
        "\n",
        "\n",
        "class TriviaQA(QABenchmark):\n",
        "    def __init__(self, split='validation'):\n",
        "        super().__init__()\n",
        "        loaded_dataset = load_dataset('trivia_qa', 'rc', split=split)\n",
        "        self.dataset = [(example['question'], list(set([example['answer']['value']] + example['answer']['aliases'])))\n",
        "                        for example in loaded_dataset]\n",
        "\n",
        "\n",
        "class Lama(QABenchmark):\n",
        "    def __init__(self, split: str = 'train'):\n",
        "        super().__init__()\n",
        "        loaded_dataset = load_dataset('lama', split=split)\n",
        "        self.dataset = [(example['masked_sentence'][:-7], example['obj_label']) for example in loaded_dataset\n",
        "                        if example['masked_sentence'][-7:] == '[MASK].']\n",
        "\n",
        "\n",
        "def get_optional_in_context_demonstrations_for_triviaqa(size: int = 200):\n",
        "  trivia_qa_train_set = TriviaQA(split='train')\n",
        "  return trivia_qa_train_set.first_k(k=size)\n",
        "\n",
        "\n",
        "def get_triviaqa_validation_set(size: int = 100):\n",
        "  trivia_qa_train_set = TriviaQA(split='validation')\n",
        "  return trivia_qa_train_set.sample(k=size)\n"
      ],
      "metadata": {
        "id": "fQNiwYJ18fAU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT2\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "def print_output(output: str):\n",
        "    print(\"Output:\\n\" + 100 * '-')\n",
        "    print(output)\n",
        "\n",
        "\n",
        "def process_generation(text: str): \n",
        "    if not text:\n",
        "        return text\n",
        "    while text and text[0] in ['\\n', ':', ' ', ',', ';']:\n",
        "        text = text[1:]\n",
        "    return text\n",
        "\n",
        "\n",
        "def load_gpt2(model_name: str = 'gpt2-medium'):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "model, tokenizer = load_gpt2()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "def sampling(input_text: str, max_length=50, temperature=0.7):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    input_ids_len = input_ids.shape[1]\n",
        "    sample_output = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        max_length=input_ids_len + max_length,\n",
        "        top_k=0,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    return process_generation(tokenizer.decode(sample_output[0][input_ids_len:], skip_special_tokens=True))\n",
        "\n",
        "\n",
        "def beam_search(input_text: str, max_length=20):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    input_ids_len = input_ids.shape[1]\n",
        "    beam_output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=input_ids_len + max_length,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True,\n",
        "        # output_scores=True,\n",
        "    )\n",
        "    return process_generation(tokenizer.decode(beam_output[0][input_ids_len:], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "okYuhuWS9OUh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "    import string, re\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        return re.sub(regex, \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "\n",
        "def check_answer_truthfulness(generated_answer, gold_answers):\n",
        "    if isinstance(gold_answers, str):\n",
        "        gold_answers = [gold_answers]\n",
        "    normalized_generation = normalize_text(generated_answer)\n",
        "    return any([normalize_text(answer) in normalized_generation for answer in gold_answers])"
      ],
      "metadata": {
        "id": "pSLQNAUFALZO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optional_in_context_demonstrations = get_optional_in_context_demonstrations_for_triviaqa(size=500)\n",
        "validation_set = get_triviaqa_validation_set(size=200)"
      ],
      "metadata": {
        "id": "h8s9FkXTMNF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# section 1 - fill in your code here"
      ],
      "metadata": {
        "id": "q8cAtxPfMr9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "def cls_pooling(model_output, attention_mask):\n",
        "    return model_output[0][:,0]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-cls-token')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-cls-token')\n",
        "\n",
        "\n",
        "def encode_question(question: str):\n",
        "  encoded_input = tokenizer([question], padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "      model_output = model(**encoded_input)\n",
        "\n",
        "  # Perform pooling. In this case, max pooling.\n",
        "  sentence_embeddings = cls_pooling(model_output, encoded_input['attention_mask'])\n",
        "  \n",
        "  return sentence_embeddings"
      ],
      "metadata": {
        "id": "B9CyZ1j4Lgls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# section 2 - fill in your code here"
      ],
      "metadata": {
        "id": "_WNA1PZqMXm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lama_validation_set = Lama().sample(200)"
      ],
      "metadata": {
        "id": "v6D_aEr_bzSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# section 3 - fill in your code here"
      ],
      "metadata": {
        "id": "ikaAqpJap-dp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}