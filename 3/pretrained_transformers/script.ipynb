{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22866,"status":"ok","timestamp":1688658372906,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"},"user_tz":-180},"id":"CDYg7I398d1h","outputId":"85077725-9845-4465-c405-1510a6312970"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1232,"status":"ok","timestamp":1688658374135,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"},"user_tz":-180},"id":"4S8k6KE98biN","outputId":"d5e085bf-1d2b-498f-f6f7-47fc5c05d79a"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/NLP_hw3/pretrained_transformers\n"]}],"source":["%cd /content/drive/MyDrive/Colab Notebooks/NLP_hw3/pretrained_transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Q3fj2k0a8Hb-","executionInfo":{"status":"ok","timestamp":1688658374136,"user_tz":-180,"elapsed":5,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"}}},"outputs":[],"source":["# !pip install colab-xterm\n","# %load_ext colabxterm\n","# %xterm"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"n6O2e0KWJt8H","executionInfo":{"status":"ok","timestamp":1688658374136,"user_tz":-180,"elapsed":4,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"}}},"outputs":[],"source":["# !tensorboard --logdir expt/."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11858,"status":"ok","timestamp":1688658385991,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"},"user_tz":-180},"id":"JGG98kAt88km","outputId":"61ccc21f-ae79-4285-b1ad-155016725a52"},"outputs":[{"output_type":"stream","name":"stdout","text":["data has 418352 characters, 256 unique.\n","x: Where was Khatchig Mouradian born?⁇Lebanon⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Lebanon⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Where was Jacob Henry Studer born?⁇Columbus⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Columbus⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Where was John Stephen born?⁇Glasgow⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: □□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Glasgow⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Where was Georgina Willis born?⁇Australia⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Australia⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"]}],"source":["!python src/dataset.py namedata"]},{"cell_type":"markdown","metadata":{"id":"1YJ9R2d9e3hk"},"source":["Finetune Not Pretrained"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":209527,"status":"ok","timestamp":1688658595515,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"},"user_tz":-180},"id":"qWbY-LdBOB8j","outputId":"4a448528-d86c-45af-a62f-5f99e08cbab0"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-06 15:46:28.186627: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-06 15:46:29.151682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","epoch 1 iter 7: train loss 3.41325. lr 5.999844e-04: 100% 8/8 [00:06<00:00,  1.30it/s]\n","epoch 2 iter 7: train loss 2.67111. lr 5.999351e-04: 100% 8/8 [00:02<00:00,  3.67it/s]\n","epoch 3 iter 7: train loss 2.35626. lr 5.998521e-04: 100% 8/8 [00:02<00:00,  3.67it/s]\n","epoch 4 iter 7: train loss 2.17262. lr 5.997352e-04: 100% 8/8 [00:02<00:00,  3.67it/s]\n","epoch 5 iter 7: train loss 2.06290. lr 5.995847e-04: 100% 8/8 [00:02<00:00,  3.57it/s]\n","epoch 6 iter 7: train loss 2.01055. lr 5.994004e-04: 100% 8/8 [00:02<00:00,  3.75it/s]\n","epoch 7 iter 7: train loss 1.94835. lr 5.991823e-04: 100% 8/8 [00:02<00:00,  3.66it/s]\n","epoch 8 iter 7: train loss 1.90859. lr 5.989306e-04: 100% 8/8 [00:02<00:00,  3.65it/s]\n","epoch 9 iter 7: train loss 1.84563. lr 5.986453e-04: 100% 8/8 [00:02<00:00,  3.76it/s]\n","epoch 10 iter 7: train loss 1.79511. lr 5.983263e-04: 100% 8/8 [00:02<00:00,  3.67it/s]\n","epoch 11 iter 7: train loss 1.72696. lr 5.979737e-04: 100% 8/8 [00:02<00:00,  3.61it/s]\n","epoch 12 iter 7: train loss 1.65895. lr 5.975876e-04: 100% 8/8 [00:02<00:00,  3.64it/s]\n","epoch 13 iter 7: train loss 1.60580. lr 5.971680e-04: 100% 8/8 [00:02<00:00,  3.69it/s]\n","epoch 14 iter 7: train loss 1.53940. lr 5.967149e-04: 100% 8/8 [00:02<00:00,  3.72it/s]\n","epoch 15 iter 7: train loss 1.44568. lr 5.962284e-04: 100% 8/8 [00:02<00:00,  3.61it/s]\n","epoch 16 iter 7: train loss 1.36922. lr 5.957086e-04: 100% 8/8 [00:02<00:00,  3.65it/s]\n","epoch 17 iter 7: train loss 1.31820. lr 5.951554e-04: 100% 8/8 [00:02<00:00,  3.65it/s]\n","epoch 18 iter 7: train loss 1.23260. lr 5.945690e-04: 100% 8/8 [00:02<00:00,  3.61it/s]\n","epoch 19 iter 7: train loss 1.14328. lr 5.939495e-04: 100% 8/8 [00:02<00:00,  3.61it/s]\n","epoch 20 iter 7: train loss 1.09362. lr 5.932969e-04: 100% 8/8 [00:02<00:00,  3.62it/s]\n","epoch 21 iter 7: train loss 1.03163. lr 5.926112e-04: 100% 8/8 [00:02<00:00,  3.60it/s]\n","epoch 22 iter 7: train loss 0.97911. lr 5.918926e-04: 100% 8/8 [00:02<00:00,  3.63it/s]\n","epoch 23 iter 7: train loss 0.97644. lr 5.911412e-04: 100% 8/8 [00:02<00:00,  3.60it/s]\n","epoch 24 iter 7: train loss 0.92699. lr 5.903569e-04: 100% 8/8 [00:02<00:00,  3.60it/s]\n","epoch 25 iter 7: train loss 0.88518. lr 5.895400e-04: 100% 8/8 [00:02<00:00,  3.58it/s]\n","epoch 26 iter 7: train loss 0.85150. lr 5.886905e-04: 100% 8/8 [00:02<00:00,  3.54it/s]\n","epoch 27 iter 7: train loss 0.80409. lr 5.878084e-04: 100% 8/8 [00:02<00:00,  3.54it/s]\n","epoch 28 iter 7: train loss 0.82314. lr 5.868940e-04: 100% 8/8 [00:02<00:00,  3.61it/s]\n","epoch 29 iter 7: train loss 0.77552. lr 5.859473e-04: 100% 8/8 [00:02<00:00,  3.62it/s]\n","epoch 30 iter 7: train loss 0.75794. lr 5.849683e-04: 100% 8/8 [00:02<00:00,  3.60it/s]\n","epoch 31 iter 7: train loss 0.76418. lr 5.839573e-04: 100% 8/8 [00:02<00:00,  3.50it/s]\n","epoch 32 iter 7: train loss 0.72572. lr 5.829143e-04: 100% 8/8 [00:02<00:00,  3.57it/s]\n","epoch 33 iter 7: train loss 0.70300. lr 5.818395e-04: 100% 8/8 [00:02<00:00,  3.60it/s]\n","epoch 34 iter 7: train loss 0.66592. lr 5.807329e-04: 100% 8/8 [00:02<00:00,  3.59it/s]\n","epoch 35 iter 7: train loss 0.64147. lr 5.795947e-04: 100% 8/8 [00:02<00:00,  3.54it/s]\n","epoch 36 iter 7: train loss 0.61750. lr 5.784251e-04: 100% 8/8 [00:02<00:00,  3.57it/s]\n","epoch 37 iter 7: train loss 0.61064. lr 5.772241e-04: 100% 8/8 [00:02<00:00,  3.49it/s]\n","epoch 38 iter 7: train loss 0.61913. lr 5.759918e-04: 100% 8/8 [00:02<00:00,  3.51it/s]\n","epoch 39 iter 7: train loss 0.58748. lr 5.747285e-04: 100% 8/8 [00:02<00:00,  3.62it/s]\n","epoch 40 iter 7: train loss 0.59138. lr 5.734343e-04: 100% 8/8 [00:02<00:00,  3.56it/s]\n","epoch 41 iter 7: train loss 0.57493. lr 5.721093e-04: 100% 8/8 [00:02<00:00,  3.53it/s]\n","epoch 42 iter 7: train loss 0.54946. lr 5.707537e-04: 100% 8/8 [00:02<00:00,  3.50it/s]\n","epoch 43 iter 7: train loss 0.54579. lr 5.693675e-04: 100% 8/8 [00:02<00:00,  3.51it/s]\n","epoch 44 iter 7: train loss 0.52687. lr 5.679511e-04: 100% 8/8 [00:02<00:00,  3.54it/s]\n","epoch 45 iter 7: train loss 0.49924. lr 5.665044e-04: 100% 8/8 [00:02<00:00,  3.51it/s]\n","epoch 46 iter 7: train loss 0.49428. lr 5.650278e-04: 100% 8/8 [00:02<00:00,  3.51it/s]\n","epoch 47 iter 7: train loss 0.50086. lr 5.635213e-04: 100% 8/8 [00:02<00:00,  3.50it/s]\n","epoch 48 iter 7: train loss 0.49254. lr 5.619852e-04: 100% 8/8 [00:02<00:00,  3.52it/s]\n","epoch 49 iter 7: train loss 0.46039. lr 5.604195e-04: 100% 8/8 [00:02<00:00,  3.51it/s]\n","epoch 50 iter 7: train loss 0.48296. lr 5.588246e-04: 100% 8/8 [00:02<00:00,  3.48it/s]\n","epoch 51 iter 7: train loss 0.45665. lr 5.572005e-04: 100% 8/8 [00:02<00:00,  3.48it/s]\n","epoch 52 iter 7: train loss 0.45810. lr 5.555474e-04: 100% 8/8 [00:02<00:00,  3.44it/s]\n","epoch 53 iter 7: train loss 0.42958. lr 5.538656e-04: 100% 8/8 [00:02<00:00,  3.49it/s]\n","epoch 54 iter 7: train loss 0.43987. lr 5.521552e-04: 100% 8/8 [00:02<00:00,  3.47it/s]\n","epoch 55 iter 7: train loss 0.43639. lr 5.504164e-04: 100% 8/8 [00:02<00:00,  3.46it/s]\n","epoch 56 iter 7: train loss 0.39330. lr 5.486494e-04: 100% 8/8 [00:02<00:00,  3.46it/s]\n","epoch 57 iter 7: train loss 0.40309. lr 5.468544e-04: 100% 8/8 [00:02<00:00,  3.46it/s]\n","epoch 58 iter 7: train loss 0.37131. lr 5.450316e-04: 100% 8/8 [00:02<00:00,  3.45it/s]\n","epoch 59 iter 7: train loss 0.35584. lr 5.431812e-04: 100% 8/8 [00:02<00:00,  3.44it/s]\n","epoch 60 iter 7: train loss 0.35965. lr 5.413034e-04: 100% 8/8 [00:02<00:00,  3.44it/s]\n","epoch 61 iter 7: train loss 0.35191. lr 5.393985e-04: 100% 8/8 [00:02<00:00,  3.45it/s]\n","epoch 62 iter 7: train loss 0.34288. lr 5.374666e-04: 100% 8/8 [00:02<00:00,  3.47it/s]\n","epoch 63 iter 7: train loss 0.31208. lr 5.355080e-04: 100% 8/8 [00:02<00:00,  3.42it/s]\n","epoch 64 iter 7: train loss 0.32789. lr 5.335229e-04: 100% 8/8 [00:02<00:00,  3.43it/s]\n","epoch 65 iter 7: train loss 0.30801. lr 5.315115e-04: 100% 8/8 [00:02<00:00,  3.43it/s]\n","epoch 66 iter 7: train loss 0.29495. lr 5.294740e-04: 100% 8/8 [00:02<00:00,  3.46it/s]\n","epoch 67 iter 7: train loss 0.28873. lr 5.274107e-04: 100% 8/8 [00:02<00:00,  3.42it/s]\n","epoch 68 iter 7: train loss 0.28922. lr 5.253217e-04: 100% 8/8 [00:02<00:00,  3.41it/s]\n","epoch 69 iter 7: train loss 0.27681. lr 5.232074e-04: 100% 8/8 [00:02<00:00,  3.38it/s]\n","epoch 70 iter 7: train loss 0.25585. lr 5.210680e-04: 100% 8/8 [00:02<00:00,  3.44it/s]\n","epoch 71 iter 7: train loss 0.24926. lr 5.189037e-04: 100% 8/8 [00:02<00:00,  3.43it/s]\n","epoch 72 iter 7: train loss 0.22439. lr 5.167147e-04: 100% 8/8 [00:02<00:00,  3.41it/s]\n","epoch 73 iter 7: train loss 0.20263. lr 5.145014e-04: 100% 8/8 [00:02<00:00,  3.38it/s]\n","epoch 74 iter 7: train loss 0.20056. lr 5.122639e-04: 100% 8/8 [00:02<00:00,  3.40it/s]\n","epoch 75 iter 7: train loss 0.21425. lr 5.100024e-04: 100% 8/8 [00:02<00:00,  3.42it/s]\n"]}],"source":["!python src/run.py finetune vanilla wiki.txt \\\n","--writing_params_path vanilla.model.params \\\n","--finetune_corpus_path birth_places_train.tsv"]},{"cell_type":"markdown","metadata":{"id":"kinJxjOaezk-"},"source":["Evaluate on the dev set, writing out predictions"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67087,"status":"ok","timestamp":1688658662598,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"},"user_tz":-180},"id":"Pfr6BgADOQGw","outputId":"3d7640c7-5ed2-4d11-95ef-ad7f2fc7ebf7"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-06 15:49:55.931221: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-06 15:49:56.929675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","500it [00:58,  8.61it/s]\n","Correct: 10.0 out of 500.0: 2.0%\n"]}],"source":["!python src/run.py evaluate vanilla wiki.txt \\\n","--reading_params_path vanilla.model.params \\\n","--eval_corpus_path birth_dev.tsv \\\n","--outputs_path vanilla.nopretrain.dev.predictions"]},{"cell_type":"markdown","metadata":{"id":"qaCTrZVRevgX"},"source":["Evaluate on the test set, writing out predictions"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60177,"status":"ok","timestamp":1688658722769,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"},"user_tz":-180},"id":"RH8bdrEzOTyz","outputId":"8b538a36-3f6b-4b58-905f-0c779f94423c"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-06 15:51:03.757491: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-06 15:51:04.749395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","437it [00:50,  8.61it/s]\n","No gold birth places provided; returning (0,0)\n","Predictions written to vanilla.nopretrain.test.predictions; no targets provided\n"]}],"source":["!python src/run.py evaluate vanilla wiki.txt \\\n","--reading_params_path vanilla.model.params \\\n","--eval_corpus_path birth_test_inputs.tsv \\\n","--outputs_path vanilla.nopretrain.test.predictions"]},{"cell_type":"markdown","source":["Baseline"],"metadata":{"id":"qGZGGnjNyanF"}},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2412,"status":"ok","timestamp":1688658725167,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"},"user_tz":-180},"id":"i2iZBnXOOV9Z","outputId":"df5ffccb-fc15-4e53-b7ad-dc5680d24f44"},"outputs":[{"output_type":"stream","name":"stdout","text":["Correct: 25.0 out of 500.0: 5.0%\n"]}],"source":["!python src/london_baseline.py"]},{"cell_type":"markdown","metadata":{"id":"QlPQjOqeu5Jg"},"source":["Example `__get_item__()`"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2257,"status":"ok","timestamp":1688658727420,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"},"user_tz":-180},"id":"1BQw8RcxOYVx","outputId":"562616c0-6d02-49d9-9f60-517f7beef5d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["data has 418352 characters, 256 unique.\n","x: Khatchig Mouradian. K⁇journalist, ⁇hatchig Mouradian is a □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: hatchig Mouradian. K⁇journalist, ⁇hatchig Mouradian is a □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Jacob Henry Stu⁇er (26 February 18⁇der. Jacob Henry Stud□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: acob Henry Stu⁇er (26 February 18⁇der. Jacob Henry Stud□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: J⁇n. Born in Gl⁇ohn Stephe□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: ⁇n. Born in Gl⁇ohn Stephe□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","x: Ge⁇orgina Willis is an award winning film director who was⁇orgina Willis. Ge□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n","y: e⁇orgina Willis is an award winning film director who was⁇orgina Willis. Ge□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"]}],"source":["!python src/dataset.py charcorruption"]},{"cell_type":"markdown","source":["Finetune Pretrained"],"metadata":{"id":"XmPp5Taey7q_"}},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2461777,"status":"ok","timestamp":1688661189194,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"},"user_tz":-180},"id":"2GXynnKbObUx","outputId":"50a1f176-89ce-45fd-94fc-e588d1fb16ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-06 15:52:08.793777: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-06 15:52:09.794605: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","epoch 1 iter 22: train loss 3.45466. lr 5.999655e-03: 100% 23/23 [00:04<00:00,  5.72it/s]\n","epoch 2 iter 22: train loss 3.20363. lr 5.998582e-03: 100% 23/23 [00:03<00:00,  6.96it/s]\n","epoch 3 iter 22: train loss 3.05656. lr 5.996780e-03: 100% 23/23 [00:03<00:00,  6.90it/s]\n","epoch 4 iter 22: train loss 2.89452. lr 5.994250e-03: 100% 23/23 [00:03<00:00,  6.84it/s]\n","epoch 5 iter 22: train loss 2.82164. lr 5.990993e-03: 100% 23/23 [00:03<00:00,  6.87it/s]\n","epoch 6 iter 22: train loss 2.73553. lr 5.987009e-03: 100% 23/23 [00:03<00:00,  6.87it/s]\n","epoch 7 iter 22: train loss 2.70839. lr 5.982299e-03: 100% 23/23 [00:03<00:00,  6.85it/s]\n","epoch 8 iter 22: train loss 2.71480. lr 5.976865e-03: 100% 23/23 [00:03<00:00,  6.76it/s]\n","epoch 9 iter 22: train loss 2.68897. lr 5.970707e-03: 100% 23/23 [00:03<00:00,  6.78it/s]\n","epoch 10 iter 22: train loss 2.67737. lr 5.963828e-03: 100% 23/23 [00:03<00:00,  6.84it/s]\n","epoch 11 iter 22: train loss 2.64746. lr 5.956228e-03: 100% 23/23 [00:03<00:00,  6.83it/s]\n","epoch 12 iter 22: train loss 2.62353. lr 5.947911e-03: 100% 23/23 [00:03<00:00,  6.82it/s]\n","epoch 13 iter 22: train loss 2.63005. lr 5.938877e-03: 100% 23/23 [00:03<00:00,  6.78it/s]\n","epoch 14 iter 22: train loss 2.62231. lr 5.929129e-03: 100% 23/23 [00:03<00:00,  6.76it/s]\n","epoch 15 iter 22: train loss 2.57966. lr 5.918669e-03: 100% 23/23 [00:03<00:00,  6.75it/s]\n","epoch 16 iter 22: train loss 2.59759. lr 5.907500e-03: 100% 23/23 [00:03<00:00,  6.71it/s]\n","epoch 17 iter 22: train loss 2.58087. lr 5.895625e-03: 100% 23/23 [00:03<00:00,  6.72it/s]\n","epoch 18 iter 22: train loss 2.56886. lr 5.883046e-03: 100% 23/23 [00:03<00:00,  6.76it/s]\n","epoch 19 iter 22: train loss 2.56185. lr 5.869767e-03: 100% 23/23 [00:03<00:00,  6.73it/s]\n","epoch 20 iter 22: train loss 2.55917. lr 5.855791e-03: 100% 23/23 [00:03<00:00,  6.70it/s]\n","epoch 21 iter 22: train loss 2.56035. lr 5.841120e-03: 100% 23/23 [00:03<00:00,  6.70it/s]\n","epoch 22 iter 22: train loss 2.55640. lr 5.825760e-03: 100% 23/23 [00:03<00:00,  6.61it/s]\n","epoch 23 iter 22: train loss 2.54202. lr 5.809713e-03: 100% 23/23 [00:03<00:00,  6.68it/s]\n","epoch 24 iter 22: train loss 2.56880. lr 5.792983e-03: 100% 23/23 [00:03<00:00,  6.70it/s]\n","epoch 25 iter 22: train loss 2.54972. lr 5.775575e-03: 100% 23/23 [00:03<00:00,  6.71it/s]\n","epoch 26 iter 22: train loss 2.52181. lr 5.757492e-03: 100% 23/23 [00:03<00:00,  6.63it/s]\n","epoch 27 iter 22: train loss 2.53633. lr 5.738739e-03: 100% 23/23 [00:03<00:00,  6.61it/s]\n","epoch 28 iter 22: train loss 2.50756. lr 5.719321e-03: 100% 23/23 [00:03<00:00,  6.64it/s]\n","epoch 29 iter 22: train loss 2.50716. lr 5.699242e-03: 100% 23/23 [00:03<00:00,  6.61it/s]\n","epoch 30 iter 22: train loss 2.49307. lr 5.678508e-03: 100% 23/23 [00:03<00:00,  6.58it/s]\n","epoch 31 iter 22: train loss 2.48036. lr 5.657122e-03: 100% 23/23 [00:03<00:00,  6.64it/s]\n","epoch 32 iter 22: train loss 2.47548. lr 5.635091e-03: 100% 23/23 [00:03<00:00,  6.66it/s]\n","epoch 33 iter 22: train loss 2.41944. lr 5.612420e-03: 100% 23/23 [00:03<00:00,  6.56it/s]\n","epoch 34 iter 22: train loss 2.44115. lr 5.589114e-03: 100% 23/23 [00:03<00:00,  6.63it/s]\n","epoch 35 iter 22: train loss 2.43417. lr 5.565180e-03: 100% 23/23 [00:03<00:00,  6.58it/s]\n","epoch 36 iter 22: train loss 2.40021. lr 5.540621e-03: 100% 23/23 [00:03<00:00,  6.54it/s]\n","epoch 37 iter 22: train loss 2.37085. lr 5.515446e-03: 100% 23/23 [00:03<00:00,  6.57it/s]\n","epoch 38 iter 22: train loss 2.40470. lr 5.489659e-03: 100% 23/23 [00:03<00:00,  6.53it/s]\n","epoch 39 iter 22: train loss 2.33169. lr 5.463268e-03: 100% 23/23 [00:03<00:00,  6.56it/s]\n","epoch 40 iter 22: train loss 2.37218. lr 5.436278e-03: 100% 23/23 [00:03<00:00,  6.55it/s]\n","epoch 41 iter 22: train loss 2.35710. lr 5.408696e-03: 100% 23/23 [00:03<00:00,  6.54it/s]\n","epoch 42 iter 22: train loss 2.31382. lr 5.380529e-03: 100% 23/23 [00:03<00:00,  6.53it/s]\n","epoch 43 iter 22: train loss 2.29710. lr 5.351784e-03: 100% 23/23 [00:03<00:00,  6.45it/s]\n","epoch 44 iter 22: train loss 2.26943. lr 5.322467e-03: 100% 23/23 [00:03<00:00,  6.48it/s]\n","epoch 45 iter 22: train loss 2.28877. lr 5.292586e-03: 100% 23/23 [00:03<00:00,  6.50it/s]\n","epoch 46 iter 22: train loss 2.23858. lr 5.262147e-03: 100% 23/23 [00:03<00:00,  6.50it/s]\n","epoch 47 iter 22: train loss 2.21473. lr 5.231160e-03: 100% 23/23 [00:03<00:00,  6.47it/s]\n","epoch 48 iter 22: train loss 2.20618. lr 5.199630e-03: 100% 23/23 [00:03<00:00,  6.49it/s]\n","epoch 49 iter 22: train loss 2.21024. lr 5.167566e-03: 100% 23/23 [00:03<00:00,  6.46it/s]\n","epoch 50 iter 22: train loss 2.19523. lr 5.134975e-03: 100% 23/23 [00:03<00:00,  6.41it/s]\n","epoch 51 iter 22: train loss 2.14805. lr 5.101865e-03: 100% 23/23 [00:03<00:00,  6.47it/s]\n","epoch 52 iter 22: train loss 2.12357. lr 5.068245e-03: 100% 23/23 [00:03<00:00,  6.44it/s]\n","epoch 53 iter 22: train loss 2.12543. lr 5.034122e-03: 100% 23/23 [00:03<00:00,  6.46it/s]\n","epoch 54 iter 22: train loss 2.10428. lr 4.999505e-03: 100% 23/23 [00:03<00:00,  6.42it/s]\n","epoch 55 iter 22: train loss 2.11186. lr 4.964402e-03: 100% 23/23 [00:03<00:00,  6.41it/s]\n","epoch 56 iter 22: train loss 2.05945. lr 4.928822e-03: 100% 23/23 [00:03<00:00,  6.43it/s]\n","epoch 57 iter 22: train loss 2.03220. lr 4.892773e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 58 iter 22: train loss 2.01947. lr 4.856265e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 59 iter 22: train loss 1.99902. lr 4.819305e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 60 iter 22: train loss 1.98447. lr 4.781904e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 61 iter 22: train loss 1.95478. lr 4.744069e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 62 iter 22: train loss 1.97716. lr 4.705811e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 63 iter 22: train loss 1.94923. lr 4.667138e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 64 iter 22: train loss 1.91720. lr 4.628060e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 65 iter 22: train loss 1.94307. lr 4.588587e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 66 iter 22: train loss 1.85098. lr 4.548728e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 67 iter 22: train loss 1.84426. lr 4.508492e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 68 iter 22: train loss 1.83156. lr 4.467890e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 69 iter 22: train loss 1.78132. lr 4.426931e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 70 iter 22: train loss 1.80185. lr 4.385626e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 71 iter 22: train loss 1.81452. lr 4.343984e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 72 iter 22: train loss 1.77626. lr 4.302015e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 73 iter 22: train loss 1.79618. lr 4.259730e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 74 iter 22: train loss 1.77713. lr 4.217139e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 75 iter 22: train loss 1.72306. lr 4.174253e-03: 100% 23/23 [00:03<00:00,  6.41it/s]\n","epoch 76 iter 22: train loss 1.69771. lr 4.131081e-03: 100% 23/23 [00:03<00:00,  6.45it/s]\n","epoch 77 iter 22: train loss 1.72274. lr 4.087634e-03: 100% 23/23 [00:03<00:00,  6.28it/s]\n","epoch 78 iter 22: train loss 1.69512. lr 4.043923e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 79 iter 22: train loss 1.70311. lr 3.999958e-03: 100% 23/23 [00:03<00:00,  6.40it/s]\n","epoch 80 iter 22: train loss 1.72847. lr 3.955750e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 81 iter 22: train loss 1.67310. lr 3.911311e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 82 iter 22: train loss 1.67950. lr 3.866649e-03: 100% 23/23 [00:03<00:00,  6.41it/s]\n","epoch 83 iter 22: train loss 1.68403. lr 3.821778e-03: 100% 23/23 [00:03<00:00,  6.40it/s]\n","epoch 84 iter 22: train loss 1.63473. lr 3.776706e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 85 iter 22: train loss 1.64297. lr 3.731446e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 86 iter 22: train loss 1.59691. lr 3.686008e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 87 iter 22: train loss 1.60119. lr 3.640403e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 88 iter 22: train loss 1.59489. lr 3.594643e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 89 iter 22: train loss 1.59649. lr 3.548738e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 90 iter 22: train loss 1.59562. lr 3.502700e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 91 iter 22: train loss 1.56133. lr 3.456540e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 92 iter 22: train loss 1.54243. lr 3.410269e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 93 iter 22: train loss 1.53351. lr 3.363899e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 94 iter 22: train loss 1.52412. lr 3.317439e-03: 100% 23/23 [00:03<00:00,  6.28it/s]\n","epoch 95 iter 22: train loss 1.50323. lr 3.270903e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 96 iter 22: train loss 1.53054. lr 3.224301e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 97 iter 22: train loss 1.53070. lr 3.177645e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 98 iter 22: train loss 1.49044. lr 3.130945e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 99 iter 22: train loss 1.48360. lr 3.084213e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 100 iter 22: train loss 1.49989. lr 3.037461e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 101 iter 22: train loss 1.51052. lr 2.990700e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 102 iter 22: train loss 1.49599. lr 2.943941e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 103 iter 22: train loss 1.44368. lr 2.897196e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 104 iter 22: train loss 1.45913. lr 2.850476e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 105 iter 22: train loss 1.44867. lr 2.803792e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 106 iter 22: train loss 1.43497. lr 2.757156e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 107 iter 22: train loss 1.42745. lr 2.710578e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 108 iter 22: train loss 1.41798. lr 2.664071e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 109 iter 22: train loss 1.42919. lr 2.617646e-03: 100% 23/23 [00:03<00:00,  6.40it/s]\n","epoch 110 iter 22: train loss 1.38456. lr 2.571314e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 111 iter 22: train loss 1.39230. lr 2.525085e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 112 iter 22: train loss 1.38490. lr 2.478973e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 113 iter 22: train loss 1.38328. lr 2.432986e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 114 iter 22: train loss 1.42615. lr 2.387138e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 115 iter 22: train loss 1.36601. lr 2.341438e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 116 iter 22: train loss 1.36111. lr 2.295898e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 117 iter 22: train loss 1.33169. lr 2.250530e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 118 iter 22: train loss 1.36136. lr 2.205343e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 119 iter 22: train loss 1.37133. lr 2.160350e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 120 iter 22: train loss 1.33987. lr 2.115561e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 121 iter 22: train loss 1.30955. lr 2.070986e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 122 iter 22: train loss 1.32167. lr 2.026637e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 123 iter 22: train loss 1.29993. lr 1.982525e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 124 iter 22: train loss 1.32691. lr 1.938660e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 125 iter 22: train loss 1.28930. lr 1.895053e-03: 100% 23/23 [00:03<00:00,  6.25it/s]\n","epoch 126 iter 22: train loss 1.28782. lr 1.851714e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 127 iter 22: train loss 1.25177. lr 1.808654e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 128 iter 22: train loss 1.24947. lr 1.765884e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 129 iter 22: train loss 1.27668. lr 1.723413e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 130 iter 22: train loss 1.25726. lr 1.681253e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 131 iter 22: train loss 1.25513. lr 1.639413e-03: 100% 23/23 [00:03<00:00,  6.28it/s]\n","epoch 132 iter 22: train loss 1.23824. lr 1.597904e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 133 iter 22: train loss 1.22003. lr 1.556735e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 134 iter 22: train loss 1.21670. lr 1.515917e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 135 iter 22: train loss 1.26609. lr 1.475460e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 136 iter 22: train loss 1.20508. lr 1.435372e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 137 iter 22: train loss 1.20981. lr 1.395666e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 138 iter 22: train loss 1.22174. lr 1.356348e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 139 iter 22: train loss 1.18534. lr 1.317431e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 140 iter 22: train loss 1.19945. lr 1.278922e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 141 iter 22: train loss 1.17691. lr 1.240831e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 142 iter 22: train loss 1.15092. lr 1.203167e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 143 iter 22: train loss 1.17832. lr 1.165941e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 144 iter 22: train loss 1.15832. lr 1.129159e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 145 iter 22: train loss 1.17845. lr 1.092833e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 146 iter 22: train loss 1.13730. lr 1.056969e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 147 iter 22: train loss 1.18056. lr 1.021578e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 148 iter 22: train loss 1.13257. lr 9.866674e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 149 iter 22: train loss 1.15480. lr 9.522459e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 150 iter 22: train loss 1.13508. lr 9.183220e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 151 iter 22: train loss 1.14154. lr 8.849039e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 152 iter 22: train loss 1.14730. lr 8.519997e-04: 100% 23/23 [00:03<00:00,  6.26it/s]\n","epoch 153 iter 22: train loss 1.12511. lr 8.196173e-04: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 154 iter 22: train loss 1.15891. lr 7.877647e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 155 iter 22: train loss 1.12584. lr 7.564495e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 156 iter 22: train loss 1.12944. lr 7.256795e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 157 iter 22: train loss 1.11401. lr 6.954620e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 158 iter 22: train loss 1.10349. lr 6.658045e-04: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 159 iter 22: train loss 1.09112. lr 6.367140e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 160 iter 22: train loss 1.08304. lr 6.081978e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 161 iter 22: train loss 1.09124. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 162 iter 22: train loss 1.05496. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 163 iter 22: train loss 1.07896. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 164 iter 22: train loss 1.10561. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 165 iter 22: train loss 1.09249. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 166 iter 22: train loss 1.07838. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 167 iter 22: train loss 1.10089. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 168 iter 22: train loss 1.06239. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 169 iter 22: train loss 1.06284. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 170 iter 22: train loss 1.08884. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 171 iter 22: train loss 1.06483. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 172 iter 22: train loss 1.04526. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 173 iter 22: train loss 1.07762. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 174 iter 22: train loss 1.06248. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 175 iter 22: train loss 1.05787. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 176 iter 22: train loss 1.07311. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 177 iter 22: train loss 1.07652. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 178 iter 22: train loss 1.03683. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 179 iter 22: train loss 1.08101. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 180 iter 22: train loss 1.04756. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 181 iter 22: train loss 1.07486. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 182 iter 22: train loss 1.08071. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 183 iter 22: train loss 1.03779. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 184 iter 22: train loss 1.06917. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 185 iter 22: train loss 1.07245. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 186 iter 22: train loss 1.06726. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 187 iter 22: train loss 1.04831. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 188 iter 22: train loss 1.01966. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 189 iter 22: train loss 1.04268. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 190 iter 22: train loss 1.04845. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 191 iter 22: train loss 1.03576. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 192 iter 22: train loss 1.02260. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 193 iter 22: train loss 1.02294. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 194 iter 22: train loss 1.04585. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.40it/s]\n","epoch 195 iter 22: train loss 1.02598. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 196 iter 22: train loss 1.01607. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 197 iter 22: train loss 1.04954. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 198 iter 22: train loss 1.03720. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 199 iter 22: train loss 1.04668. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 200 iter 22: train loss 1.02103. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 201 iter 22: train loss 1.02514. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 202 iter 22: train loss 1.00250. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 203 iter 22: train loss 1.02383. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 204 iter 22: train loss 1.03240. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 205 iter 22: train loss 0.99472. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 206 iter 22: train loss 1.00959. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 207 iter 22: train loss 1.01511. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 208 iter 22: train loss 1.02650. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 209 iter 22: train loss 1.03209. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 210 iter 22: train loss 0.98950. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.41it/s]\n","epoch 211 iter 22: train loss 1.01567. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 212 iter 22: train loss 1.02819. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 213 iter 22: train loss 1.02781. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.41it/s]\n","epoch 214 iter 22: train loss 1.04620. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 215 iter 22: train loss 0.99085. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 216 iter 22: train loss 1.03092. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 217 iter 22: train loss 1.00028. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 218 iter 22: train loss 1.01873. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 219 iter 22: train loss 1.00730. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 220 iter 22: train loss 0.98621. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 221 iter 22: train loss 1.02021. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 222 iter 22: train loss 0.98920. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 223 iter 22: train loss 0.99197. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 224 iter 22: train loss 0.99818. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 225 iter 22: train loss 0.95885. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.28it/s]\n","epoch 226 iter 22: train loss 1.01072. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 227 iter 22: train loss 1.00250. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 228 iter 22: train loss 1.00466. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 229 iter 22: train loss 0.99332. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 230 iter 22: train loss 0.95201. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 231 iter 22: train loss 0.96394. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 232 iter 22: train loss 1.00641. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 233 iter 22: train loss 1.00255. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 234 iter 22: train loss 0.99280. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 235 iter 22: train loss 0.96218. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.27it/s]\n","epoch 236 iter 22: train loss 1.00003. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 237 iter 22: train loss 0.98675. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 238 iter 22: train loss 0.97496. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 239 iter 22: train loss 0.97122. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 240 iter 22: train loss 0.98280. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 241 iter 22: train loss 0.98770. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 242 iter 22: train loss 0.97955. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 243 iter 22: train loss 0.97585. lr 6.039814e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 244 iter 22: train loss 0.96681. lr 6.324111e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 245 iter 22: train loss 0.99315. lr 6.614161e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 246 iter 22: train loss 0.95825. lr 6.909893e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 247 iter 22: train loss 0.98797. lr 7.211234e-04: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 248 iter 22: train loss 0.96938. lr 7.518113e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 249 iter 22: train loss 0.96087. lr 7.830454e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 250 iter 22: train loss 0.94973. lr 8.148181e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 251 iter 22: train loss 0.96774. lr 8.471217e-04: 100% 23/23 [00:03<00:00,  6.40it/s]\n","epoch 252 iter 22: train loss 0.95102. lr 8.799483e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 253 iter 22: train loss 0.98994. lr 9.132901e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 254 iter 22: train loss 0.94575. lr 9.471389e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 255 iter 22: train loss 0.95505. lr 9.814864e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 256 iter 22: train loss 0.99532. lr 1.016324e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 257 iter 22: train loss 0.93260. lr 1.051644e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 258 iter 22: train loss 1.02255. lr 1.087438e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 259 iter 22: train loss 1.00489. lr 1.123695e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 260 iter 22: train loss 0.99686. lr 1.160409e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 261 iter 22: train loss 0.98424. lr 1.197570e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 262 iter 22: train loss 0.95628. lr 1.235169e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 263 iter 22: train loss 0.96303. lr 1.273196e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 264 iter 22: train loss 0.95945. lr 1.311643e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 265 iter 22: train loss 0.96905. lr 1.350500e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 266 iter 22: train loss 0.99447. lr 1.389758e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 267 iter 22: train loss 0.96350. lr 1.429408e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 268 iter 22: train loss 0.99329. lr 1.469438e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 269 iter 22: train loss 0.97096. lr 1.509841e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 270 iter 22: train loss 1.00694. lr 1.550606e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 271 iter 22: train loss 0.97689. lr 1.591723e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 272 iter 22: train loss 1.00834. lr 1.633182e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 273 iter 22: train loss 0.99648. lr 1.674973e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 274 iter 22: train loss 0.96891. lr 1.717086e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 275 iter 22: train loss 0.97384. lr 1.759511e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 276 iter 22: train loss 1.01773. lr 1.802237e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 277 iter 22: train loss 0.98128. lr 1.845254e-03: 100% 23/23 [00:03<00:00,  6.41it/s]\n","epoch 278 iter 22: train loss 0.96649. lr 1.888552e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 279 iter 22: train loss 0.97901. lr 1.932119e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 280 iter 22: train loss 1.02870. lr 1.975947e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 281 iter 22: train loss 0.98328. lr 2.020023e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 282 iter 22: train loss 1.02057. lr 2.064337e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 283 iter 22: train loss 1.00421. lr 2.108878e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 284 iter 22: train loss 1.00077. lr 2.153636e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 285 iter 22: train loss 0.97046. lr 2.198600e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 286 iter 22: train loss 0.98733. lr 2.243758e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 287 iter 22: train loss 1.01157. lr 2.289100e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 288 iter 22: train loss 0.98546. lr 2.334615e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 289 iter 22: train loss 1.00108. lr 2.380291e-03: 100% 23/23 [00:03<00:00,  6.28it/s]\n","epoch 290 iter 22: train loss 1.00897. lr 2.426118e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 291 iter 22: train loss 0.99377. lr 2.472084e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 292 iter 22: train loss 0.98544. lr 2.518179e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 293 iter 22: train loss 0.97734. lr 2.564391e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 294 iter 22: train loss 0.97531. lr 2.610708e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 295 iter 22: train loss 0.99944. lr 2.657120e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 296 iter 22: train loss 1.02063. lr 2.703616e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 297 iter 22: train loss 1.02098. lr 2.750183e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 298 iter 22: train loss 1.00621. lr 2.796811e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 299 iter 22: train loss 1.01634. lr 2.843489e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 300 iter 22: train loss 1.02460. lr 2.890205e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 301 iter 22: train loss 1.02747. lr 2.936947e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 302 iter 22: train loss 1.00148. lr 2.983704e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 303 iter 22: train loss 1.01366. lr 3.030466e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 304 iter 22: train loss 1.00478. lr 3.077220e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 305 iter 22: train loss 1.00357. lr 3.123955e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 306 iter 22: train loss 1.00355. lr 3.170660e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 307 iter 22: train loss 0.99749. lr 3.217324e-03: 100% 23/23 [00:03<00:00,  6.26it/s]\n","epoch 308 iter 22: train loss 1.01776. lr 3.263935e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 309 iter 22: train loss 0.99977. lr 3.310482e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 310 iter 22: train loss 1.01985. lr 3.356953e-03: 100% 23/23 [00:03<00:00,  6.41it/s]\n","epoch 311 iter 22: train loss 1.02392. lr 3.403338e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 312 iter 22: train loss 1.01427. lr 3.449625e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 313 iter 22: train loss 1.02908. lr 3.495802e-03: 100% 23/23 [00:03<00:00,  6.25it/s]\n","epoch 314 iter 22: train loss 1.04657. lr 3.541859e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 315 iter 22: train loss 1.00217. lr 3.587784e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 316 iter 22: train loss 1.00291. lr 3.633567e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 317 iter 22: train loss 1.02123. lr 3.679195e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 318 iter 22: train loss 1.02236. lr 3.724659e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 319 iter 22: train loss 1.05500. lr 3.769947e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 320 iter 22: train loss 1.05559. lr 3.815047e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 321 iter 22: train loss 1.03563. lr 3.859949e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 322 iter 22: train loss 0.98861. lr 3.904643e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 323 iter 22: train loss 1.04589. lr 3.949117e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 324 iter 22: train loss 1.01263. lr 3.993360e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 325 iter 22: train loss 0.99471. lr 4.037361e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 326 iter 22: train loss 1.02282. lr 4.081111e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 327 iter 22: train loss 1.00821. lr 4.124598e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 328 iter 22: train loss 1.00024. lr 4.167812e-03: 100% 23/23 [00:03<00:00,  6.40it/s]\n","epoch 329 iter 22: train loss 1.04048. lr 4.210742e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 330 iter 22: train loss 0.99950. lr 4.253378e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 331 iter 22: train loss 0.98889. lr 4.295709e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 332 iter 22: train loss 1.01792. lr 4.337726e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 333 iter 22: train loss 1.00571. lr 4.379417e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 334 iter 22: train loss 1.00174. lr 4.420774e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 335 iter 22: train loss 1.01015. lr 4.461785e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 336 iter 22: train loss 0.99140. lr 4.502441e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 337 iter 22: train loss 1.05692. lr 4.542732e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 338 iter 22: train loss 1.03666. lr 4.582648e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 339 iter 22: train loss 1.00506. lr 4.622180e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 340 iter 22: train loss 1.03747. lr 4.661317e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 341 iter 22: train loss 1.02231. lr 4.700051e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 342 iter 22: train loss 1.01105. lr 4.738372e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 343 iter 22: train loss 1.01022. lr 4.776271e-03: 100% 23/23 [00:03<00:00,  6.27it/s]\n","epoch 344 iter 22: train loss 0.98509. lr 4.813738e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 345 iter 22: train loss 1.00998. lr 4.850764e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 346 iter 22: train loss 1.01708. lr 4.887340e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 347 iter 22: train loss 1.02148. lr 4.923459e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 348 iter 22: train loss 0.96229. lr 4.959109e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 349 iter 22: train loss 1.01541. lr 4.994284e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 350 iter 22: train loss 1.03848. lr 5.028974e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 351 iter 22: train loss 0.99332. lr 5.063172e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 352 iter 22: train loss 1.04785. lr 5.096868e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 353 iter 22: train loss 0.98139. lr 5.130054e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 354 iter 22: train loss 0.99646. lr 5.162723e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 355 iter 22: train loss 1.00220. lr 5.194867e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 356 iter 22: train loss 0.99578. lr 5.226477e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 357 iter 22: train loss 0.99314. lr 5.257546e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 358 iter 22: train loss 0.99491. lr 5.288067e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 359 iter 22: train loss 1.01129. lr 5.318032e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 360 iter 22: train loss 0.96486. lr 5.347434e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 361 iter 22: train loss 0.97696. lr 5.376265e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 362 iter 22: train loss 0.98956. lr 5.404519e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 363 iter 22: train loss 0.99366. lr 5.432189e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 364 iter 22: train loss 0.98288. lr 5.459268e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 365 iter 22: train loss 0.96689. lr 5.485749e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 366 iter 22: train loss 0.98253. lr 5.511627e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 367 iter 22: train loss 0.95398. lr 5.536894e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 368 iter 22: train loss 0.98951. lr 5.561545e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 369 iter 22: train loss 0.98539. lr 5.585574e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 370 iter 22: train loss 0.98569. lr 5.608974e-03: 100% 23/23 [00:03<00:00,  6.25it/s]\n","epoch 371 iter 22: train loss 0.95927. lr 5.631740e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 372 iter 22: train loss 0.95863. lr 5.653867e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 373 iter 22: train loss 1.00410. lr 5.675350e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 374 iter 22: train loss 0.93859. lr 5.696182e-03: 100% 23/23 [00:03<00:00,  6.27it/s]\n","epoch 375 iter 22: train loss 0.97572. lr 5.716359e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 376 iter 22: train loss 0.98465. lr 5.735876e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 377 iter 22: train loss 0.99063. lr 5.754729e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 378 iter 22: train loss 0.94505. lr 5.772912e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 379 iter 22: train loss 0.95036. lr 5.790422e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 380 iter 22: train loss 0.94897. lr 5.807253e-03: 100% 23/23 [00:03<00:00,  6.27it/s]\n","epoch 381 iter 22: train loss 0.97901. lr 5.823403e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 382 iter 22: train loss 0.94723. lr 5.838866e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 383 iter 22: train loss 0.94003. lr 5.853640e-03: 100% 23/23 [00:03<00:00,  6.28it/s]\n","epoch 384 iter 22: train loss 0.94141. lr 5.867720e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 385 iter 22: train loss 0.94088. lr 5.881104e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 386 iter 22: train loss 0.93860. lr 5.893788e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 387 iter 22: train loss 0.97637. lr 5.905768e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 388 iter 22: train loss 0.94016. lr 5.917043e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 389 iter 22: train loss 0.94831. lr 5.927609e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 390 iter 22: train loss 0.99050. lr 5.937464e-03: 100% 23/23 [00:03<00:00,  6.27it/s]\n","epoch 391 iter 22: train loss 0.95126. lr 5.946605e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 392 iter 22: train loss 0.93507. lr 5.955030e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 393 iter 22: train loss 0.95956. lr 5.962737e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 394 iter 22: train loss 0.95700. lr 5.969724e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 395 iter 22: train loss 0.95071. lr 5.975990e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 396 iter 22: train loss 0.94862. lr 5.981532e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 397 iter 22: train loss 0.93203. lr 5.986350e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 398 iter 22: train loss 0.94638. lr 5.990443e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 399 iter 22: train loss 0.94743. lr 5.993809e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 400 iter 22: train loss 0.94327. lr 5.996448e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 401 iter 22: train loss 0.92926. lr 5.998359e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 402 iter 22: train loss 0.94585. lr 5.999541e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 403 iter 22: train loss 0.92096. lr 5.999995e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 404 iter 22: train loss 0.93916. lr 5.999719e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 405 iter 22: train loss 0.94907. lr 5.998715e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 406 iter 22: train loss 0.89667. lr 5.996982e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 407 iter 22: train loss 0.93712. lr 5.994521e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 408 iter 22: train loss 0.94820. lr 5.991333e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 409 iter 22: train loss 0.91976. lr 5.987417e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 410 iter 22: train loss 0.94589. lr 5.982776e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 411 iter 22: train loss 0.93461. lr 5.977411e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 412 iter 22: train loss 0.93444. lr 5.971321e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 413 iter 22: train loss 0.90929. lr 5.964510e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 414 iter 22: train loss 0.91841. lr 5.956979e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 415 iter 22: train loss 0.91372. lr 5.948729e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 416 iter 22: train loss 0.93504. lr 5.939763e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 417 iter 22: train loss 0.88144. lr 5.930082e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 418 iter 22: train loss 0.89320. lr 5.919690e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 419 iter 22: train loss 0.92115. lr 5.908588e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 420 iter 22: train loss 0.90290. lr 5.896780e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 421 iter 22: train loss 0.88423. lr 5.884268e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 422 iter 22: train loss 0.93142. lr 5.871055e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 423 iter 22: train loss 0.91177. lr 5.857144e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 424 iter 22: train loss 0.93109. lr 5.842540e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 425 iter 22: train loss 0.91696. lr 5.827244e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 426 iter 22: train loss 0.86561. lr 5.811262e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 427 iter 22: train loss 0.90038. lr 5.794597e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 428 iter 22: train loss 0.87706. lr 5.777252e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 429 iter 22: train loss 0.87610. lr 5.759233e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 430 iter 22: train loss 0.87432. lr 5.740544e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 431 iter 22: train loss 0.85291. lr 5.721189e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 432 iter 22: train loss 0.86439. lr 5.701172e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 433 iter 22: train loss 0.88146. lr 5.680500e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 434 iter 22: train loss 0.87258. lr 5.659176e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 435 iter 22: train loss 0.89290. lr 5.637206e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 436 iter 22: train loss 0.89760. lr 5.614595e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 437 iter 22: train loss 0.84492. lr 5.591349e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 438 iter 22: train loss 0.88858. lr 5.567473e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 439 iter 22: train loss 0.88746. lr 5.542974e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 440 iter 22: train loss 0.86521. lr 5.517857e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 441 iter 22: train loss 0.86802. lr 5.492128e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 442 iter 22: train loss 0.86440. lr 5.465793e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 443 iter 22: train loss 0.84995. lr 5.438860e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 444 iter 22: train loss 0.81964. lr 5.411334e-03: 100% 23/23 [00:03<00:00,  6.27it/s]\n","epoch 445 iter 22: train loss 0.83889. lr 5.383222e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 446 iter 22: train loss 0.85609. lr 5.354531e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 447 iter 22: train loss 0.85268. lr 5.325268e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 448 iter 22: train loss 0.81550. lr 5.295440e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 449 iter 22: train loss 0.84993. lr 5.265054e-03: 100% 23/23 [00:03<00:00,  6.40it/s]\n","epoch 450 iter 22: train loss 0.88384. lr 5.234118e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 451 iter 22: train loss 0.82994. lr 5.202639e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 452 iter 22: train loss 0.85239. lr 5.170625e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 453 iter 22: train loss 0.83946. lr 5.138084e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 454 iter 22: train loss 0.84306. lr 5.105023e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 455 iter 22: train loss 0.83772. lr 5.071451e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 456 iter 22: train loss 0.78868. lr 5.037375e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 457 iter 22: train loss 0.77592. lr 5.002805e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 458 iter 22: train loss 0.80031. lr 4.967748e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 459 iter 22: train loss 0.83504. lr 4.932212e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 460 iter 22: train loss 0.78928. lr 4.896208e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 461 iter 22: train loss 0.78394. lr 4.859742e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 462 iter 22: train loss 0.80798. lr 4.822825e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 463 iter 22: train loss 0.80377. lr 4.785465e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 464 iter 22: train loss 0.80118. lr 4.747671e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 465 iter 22: train loss 0.76880. lr 4.709453e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 466 iter 22: train loss 0.78091. lr 4.670819e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 467 iter 22: train loss 0.78186. lr 4.631779e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 468 iter 22: train loss 0.81752. lr 4.592343e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 469 iter 22: train loss 0.81218. lr 4.552519e-03: 100% 23/23 [00:03<00:00,  6.40it/s]\n","epoch 470 iter 22: train loss 0.78826. lr 4.512319e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 471 iter 22: train loss 0.76948. lr 4.471751e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 472 iter 22: train loss 0.82432. lr 4.430826e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 473 iter 22: train loss 0.76354. lr 4.389553e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 474 iter 22: train loss 0.77834. lr 4.347942e-03: 100% 23/23 [00:03<00:00,  6.28it/s]\n","epoch 475 iter 22: train loss 0.75812. lr 4.306004e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 476 iter 22: train loss 0.75947. lr 4.263749e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 477 iter 22: train loss 0.76808. lr 4.221186e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 478 iter 22: train loss 0.76371. lr 4.178327e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 479 iter 22: train loss 0.74663. lr 4.135182e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 480 iter 22: train loss 0.78117. lr 4.091761e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 481 iter 22: train loss 0.75892. lr 4.048074e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 482 iter 22: train loss 0.78557. lr 4.004133e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 483 iter 22: train loss 0.73960. lr 3.959948e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 484 iter 22: train loss 0.72051. lr 3.915530e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 485 iter 22: train loss 0.74349. lr 3.870889e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 486 iter 22: train loss 0.75858. lr 3.826036e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 487 iter 22: train loss 0.73943. lr 3.780983e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 488 iter 22: train loss 0.70248. lr 3.735741e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 489 iter 22: train loss 0.70844. lr 3.690319e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 490 iter 22: train loss 0.71626. lr 3.644730e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 491 iter 22: train loss 0.72492. lr 3.598984e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 492 iter 22: train loss 0.72087. lr 3.553092e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 493 iter 22: train loss 0.73130. lr 3.507067e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 494 iter 22: train loss 0.72197. lr 3.460918e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 495 iter 22: train loss 0.71300. lr 3.414656e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 496 iter 22: train loss 0.67226. lr 3.368295e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 497 iter 22: train loss 0.68318. lr 3.321843e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 498 iter 22: train loss 0.71479. lr 3.275314e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 499 iter 22: train loss 0.67466. lr 3.228718e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 500 iter 22: train loss 0.71938. lr 3.182066e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 501 iter 22: train loss 0.69668. lr 3.135370e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 502 iter 22: train loss 0.69529. lr 3.088641e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 503 iter 22: train loss 0.69695. lr 3.041890e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 504 iter 22: train loss 0.69305. lr 2.995129e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 505 iter 22: train loss 0.70440. lr 2.948370e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 506 iter 22: train loss 0.68397. lr 2.901623e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 507 iter 22: train loss 0.66241. lr 2.854900e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 508 iter 22: train loss 0.67521. lr 2.808212e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 509 iter 22: train loss 0.64095. lr 2.761571e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 510 iter 22: train loss 0.65015. lr 2.714987e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 511 iter 22: train loss 0.67936. lr 2.668473e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 512 iter 22: train loss 0.64542. lr 2.622040e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 513 iter 22: train loss 0.68310. lr 2.575698e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 514 iter 22: train loss 0.64962. lr 2.529459e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 515 iter 22: train loss 0.68158. lr 2.483335e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 516 iter 22: train loss 0.66643. lr 2.437336e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 517 iter 22: train loss 0.66158. lr 2.391474e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 518 iter 22: train loss 0.66653. lr 2.345760e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 519 iter 22: train loss 0.63453. lr 2.300205e-03: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 520 iter 22: train loss 0.61383. lr 2.254820e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 521 iter 22: train loss 0.64369. lr 2.209615e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 522 iter 22: train loss 0.62585. lr 2.164603e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 523 iter 22: train loss 0.64599. lr 2.119794e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 524 iter 22: train loss 0.65070. lr 2.075199e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 525 iter 22: train loss 0.62132. lr 2.030828e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 526 iter 22: train loss 0.62864. lr 1.986693e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 527 iter 22: train loss 0.62729. lr 1.942804e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 528 iter 22: train loss 0.61124. lr 1.899172e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 529 iter 22: train loss 0.62399. lr 1.855807e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 530 iter 22: train loss 0.61638. lr 1.812721e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 531 iter 22: train loss 0.58656. lr 1.769922e-03: 100% 23/23 [00:03<00:00,  6.26it/s]\n","epoch 532 iter 22: train loss 0.62389. lr 1.727423e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 533 iter 22: train loss 0.58551. lr 1.685233e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 534 iter 22: train loss 0.60074. lr 1.643362e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 535 iter 22: train loss 0.61921. lr 1.601821e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 536 iter 22: train loss 0.57805. lr 1.560620e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 537 iter 22: train loss 0.58790. lr 1.519768e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 538 iter 22: train loss 0.60046. lr 1.479276e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 539 iter 22: train loss 0.54972. lr 1.439153e-03: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 540 iter 22: train loss 0.56564. lr 1.399410e-03: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 541 iter 22: train loss 0.56580. lr 1.360056e-03: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 542 iter 22: train loss 0.58074. lr 1.321100e-03: 100% 23/23 [00:03<00:00,  6.25it/s]\n","epoch 543 iter 22: train loss 0.56019. lr 1.282551e-03: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 544 iter 22: train loss 0.57507. lr 1.244421e-03: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 545 iter 22: train loss 0.57175. lr 1.206716e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 546 iter 22: train loss 0.55940. lr 1.169448e-03: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 547 iter 22: train loss 0.57562. lr 1.132624e-03: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 548 iter 22: train loss 0.52645. lr 1.096254e-03: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 549 iter 22: train loss 0.58289. lr 1.060346e-03: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 550 iter 22: train loss 0.54797. lr 1.024910e-03: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 551 iter 22: train loss 0.54073. lr 9.899532e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 552 iter 22: train loss 0.54332. lr 9.554851e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 553 iter 22: train loss 0.51399. lr 9.215137e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 554 iter 22: train loss 0.54685. lr 8.880473e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 555 iter 22: train loss 0.54269. lr 8.550941e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 556 iter 22: train loss 0.59651. lr 8.226619e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 557 iter 22: train loss 0.54615. lr 7.907588e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 558 iter 22: train loss 0.57124. lr 7.593924e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 559 iter 22: train loss 0.54056. lr 7.285704e-04: 100% 23/23 [00:03<00:00,  6.27it/s]\n","epoch 560 iter 22: train loss 0.54793. lr 6.983003e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 561 iter 22: train loss 0.53062. lr 6.685894e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 562 iter 22: train loss 0.52878. lr 6.394449e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 563 iter 22: train loss 0.53376. lr 6.108740e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 564 iter 22: train loss 0.57686. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 565 iter 22: train loss 0.54066. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 566 iter 22: train loss 0.54992. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 567 iter 22: train loss 0.50365. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 568 iter 22: train loss 0.50148. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 569 iter 22: train loss 0.52739. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 570 iter 22: train loss 0.52788. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 571 iter 22: train loss 0.52042. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 572 iter 22: train loss 0.55280. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 573 iter 22: train loss 0.52329. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 574 iter 22: train loss 0.56961. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 575 iter 22: train loss 0.53455. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 576 iter 22: train loss 0.53048. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 577 iter 22: train loss 0.53541. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 578 iter 22: train loss 0.52343. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 579 iter 22: train loss 0.51194. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 580 iter 22: train loss 0.52485. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 581 iter 22: train loss 0.52671. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 582 iter 22: train loss 0.50226. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 583 iter 22: train loss 0.54985. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 584 iter 22: train loss 0.49706. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 585 iter 22: train loss 0.52825. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 586 iter 22: train loss 0.50669. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 587 iter 22: train loss 0.52219. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 588 iter 22: train loss 0.52934. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 589 iter 22: train loss 0.52553. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 590 iter 22: train loss 0.51272. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 591 iter 22: train loss 0.52434. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 592 iter 22: train loss 0.50588. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.27it/s]\n","epoch 593 iter 22: train loss 0.49194. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 594 iter 22: train loss 0.54058. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 595 iter 22: train loss 0.51972. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 596 iter 22: train loss 0.51802. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.28it/s]\n","epoch 597 iter 22: train loss 0.50166. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 598 iter 22: train loss 0.50649. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 599 iter 22: train loss 0.48161. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.29it/s]\n","epoch 600 iter 22: train loss 0.51723. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 601 iter 22: train loss 0.50282. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 602 iter 22: train loss 0.48912. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 603 iter 22: train loss 0.50089. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 604 iter 22: train loss 0.51649. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 605 iter 22: train loss 0.51350. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 606 iter 22: train loss 0.51472. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.28it/s]\n","epoch 607 iter 22: train loss 0.50783. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 608 iter 22: train loss 0.50964. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 609 iter 22: train loss 0.48744. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 610 iter 22: train loss 0.49522. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 611 iter 22: train loss 0.50593. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 612 iter 22: train loss 0.48534. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 613 iter 22: train loss 0.52724. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 614 iter 22: train loss 0.51912. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.40it/s]\n","epoch 615 iter 22: train loss 0.52002. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 616 iter 22: train loss 0.53251. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 617 iter 22: train loss 0.52589. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 618 iter 22: train loss 0.51243. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 619 iter 22: train loss 0.50806. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 620 iter 22: train loss 0.49387. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 621 iter 22: train loss 0.48330. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 622 iter 22: train loss 0.50192. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 623 iter 22: train loss 0.50357. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 624 iter 22: train loss 0.48590. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 625 iter 22: train loss 0.51561. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 626 iter 22: train loss 0.48980. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 627 iter 22: train loss 0.49468. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 628 iter 22: train loss 0.48701. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 629 iter 22: train loss 0.51905. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n","epoch 630 iter 22: train loss 0.47911. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 631 iter 22: train loss 0.51425. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 632 iter 22: train loss 0.51398. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.30it/s]\n","epoch 633 iter 22: train loss 0.48883. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 634 iter 22: train loss 0.49082. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.39it/s]\n","epoch 635 iter 22: train loss 0.50799. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 636 iter 22: train loss 0.48420. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 637 iter 22: train loss 0.50149. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 638 iter 22: train loss 0.52981. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.37it/s]\n","epoch 639 iter 22: train loss 0.50345. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.28it/s]\n","epoch 640 iter 22: train loss 0.50745. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 641 iter 22: train loss 0.52299. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 642 iter 22: train loss 0.51662. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.33it/s]\n","epoch 643 iter 22: train loss 0.49506. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 644 iter 22: train loss 0.53490. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.34it/s]\n","epoch 645 iter 22: train loss 0.52127. lr 6.000000e-04: 100% 23/23 [00:03<00:00,  6.35it/s]\n","epoch 646 iter 22: train loss 0.49830. lr 6.013186e-04: 100% 23/23 [00:03<00:00,  6.31it/s]\n","epoch 647 iter 22: train loss 0.48995. lr 6.296935e-04: 100% 23/23 [00:03<00:00,  6.38it/s]\n","epoch 648 iter 22: train loss 0.50693. lr 6.586443e-04: 100% 23/23 [00:03<00:00,  6.36it/s]\n","epoch 649 iter 22: train loss 0.50975. lr 6.881640e-04: 100% 23/23 [00:03<00:00,  6.26it/s]\n","epoch 650 iter 22: train loss 0.55051. lr 7.182453e-04: 100% 23/23 [00:03<00:00,  6.32it/s]\n"]}],"source":["!python src/run.py pretrain vanilla wiki.txt \\\n","        --writing_params_path vanilla.pretrain.params"]},{"cell_type":"markdown","source":["Finetune"],"metadata":{"id":"E-M3kUubzfwg"}},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rKWiGJfzOfD5","outputId":"d618019f-fa00-474c-fdc0-b15fc47a5f7f","executionInfo":{"status":"ok","timestamp":1688661225118,"user_tz":-180,"elapsed":35927,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-06 16:33:10.290147: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-06 16:33:11.269567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","epoch 1 iter 7: train loss 0.73985. lr 5.999844e-04: 100% 8/8 [00:03<00:00,  2.57it/s]\n","epoch 2 iter 7: train loss 0.58243. lr 5.999351e-04: 100% 8/8 [00:02<00:00,  3.34it/s]\n","epoch 3 iter 7: train loss 0.51343. lr 5.998521e-04: 100% 8/8 [00:02<00:00,  3.35it/s]\n","epoch 4 iter 7: train loss 0.45722. lr 5.997352e-04: 100% 8/8 [00:02<00:00,  3.31it/s]\n","epoch 5 iter 7: train loss 0.40381. lr 5.995847e-04: 100% 8/8 [00:02<00:00,  3.29it/s]\n","epoch 6 iter 7: train loss 0.34905. lr 5.994004e-04: 100% 8/8 [00:02<00:00,  3.28it/s]\n","epoch 7 iter 7: train loss 0.28652. lr 5.991823e-04: 100% 8/8 [00:02<00:00,  3.25it/s]\n","epoch 8 iter 7: train loss 0.26362. lr 5.989306e-04: 100% 8/8 [00:02<00:00,  3.27it/s]\n","epoch 9 iter 7: train loss 0.21776. lr 5.986453e-04: 100% 8/8 [00:02<00:00,  3.17it/s]\n","epoch 10 iter 7: train loss 0.18436. lr 5.983263e-04: 100% 8/8 [00:02<00:00,  3.22it/s]\n"]}],"source":["!python src/run.py finetune vanilla wiki.txt \\\n","        --reading_params_path vanilla.pretrain.params \\\n","        --writing_params_path vanilla.finetune.params \\\n","        --finetune_corpus_path birth_places_train.tsv"]},{"cell_type":"markdown","source":["Dev"],"metadata":{"id":"ahGbsXu8zcyU"}},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4xVrx_32OlO2","executionInfo":{"status":"ok","timestamp":1688661291006,"user_tz":-180,"elapsed":65898,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"}},"outputId":"43b792f5-6fe1-44da-b4b4-b045fccff875"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-06 16:33:46.284414: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-06 16:33:47.261134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","500it [00:56,  8.81it/s]\n","Correct: 78.0 out of 500.0: 15.6%\n"]}],"source":["!python src/run.py evaluate vanilla wiki.txt  \\\n","        --reading_params_path vanilla.finetune.params \\\n","        --eval_corpus_path birth_dev.tsv \\\n","        --outputs_path vanilla.pretrain.dev.predictions"]},{"cell_type":"markdown","source":["Test"],"metadata":{"id":"2YDYMbMpzeGW"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"1TzzZZhJOl9s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688661350144,"user_tz":-180,"elapsed":59148,"user":{"displayName":"Yoni Tsur","userId":"09544362338457050917"}},"outputId":"44c0e59f-4de7-41cd-dadc-853175ec9633"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-06 16:34:52.064160: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-06 16:34:53.061911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","data has 418352 characters, 256 unique.\n","number of parameters: 3323392\n","437it [00:50,  8.74it/s]\n","No gold birth places provided; returning (0,0)\n","Predictions written to vanilla.pretrain.test.predictions; no targets provided\n"]}],"source":["!python src/run.py evaluate vanilla wiki.txt  \\\n","        --reading_params_path vanilla.finetune.params \\\n","        --eval_corpus_path birth_test_inputs.tsv \\\n","        --outputs_path vanilla.pretrain.test.predictions"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}